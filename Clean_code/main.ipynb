{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pre_processing as pp\n",
    "from arm import generate_itemsets, compute_ar_rules, get_arm_data\n",
    "from ear import compute_ear_rules\n",
    "from nn_rules import compute_nn_rules, get_rules\n",
    "from layout import create_layout\n",
    "from warehouse import Warehouse\n",
    "from articles import create_articles\n",
    "from word_analysis import calculate_word_scores\n",
    "from greedy import Greedy\n",
    "from ga import GeneticModel\n",
    "import training_methods as tm\n",
    "from parallel_processing import fast_vectorize_data\n",
    "from evaluation import evaluate_solution_for_greedy, evaluate_solution, evaluate_solution_for_greedy_no_depot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_orders = 20000\n",
    "def create_hp_tuning_data(cv_index, data_path):\n",
    "\n",
    "    data_1_2, data_3, data_4 = pp.train_val_test_split(data_path=data_path, \n",
    "                                                       val_prop=0.25, test_prop=0.25, \n",
    "                                                       n_orders=n_orders*4)\n",
    "\n",
    "    data_1, data_2, data_22 = pp.train_val_test_split(data=data_1_2, \n",
    "                                                     val_prop=0.25, test_prop=0.25, \n",
    "                                                     n_orders=n_orders*2)\n",
    "\n",
    "    data_2 = pd.concat([data_2, data_22], ignore_index=True, axis=0)\n",
    "\n",
    "                                    \n",
    "    \n",
    "\n",
    "    if cv_index == 0:\n",
    "        data = data_1.reset_index().drop('index', axis=1)\n",
    "        \n",
    "    elif cv_index == 1:\n",
    "        data = data_2.reset_index().drop('index', axis=1)\n",
    "\n",
    "    elif cv_index == 2:\n",
    "        data = data_3.reset_index().drop('index', axis=1)\n",
    "\n",
    "    elif cv_index == 3:\n",
    "        data = data_4.reset_index().drop('index', axis=1)\n",
    "    train_data, val_data, test_data = pp.train_val_test_split(data=data, \n",
    "                                                              val_prop=0.25, test_prop=0.25, \n",
    "                                                              n_orders=n_orders)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data = pd.concat([train_data, val_data], ignore_index=True, axis=0)                             \n",
    "    \n",
    "    train_data = pp.preprocess_data(train_data)\n",
    "    test_data = pp.preprocess_data(test_data)\n",
    "\n",
    "\n",
    "    train_orders = pp.unique_orders(train_data)\n",
    "    test_orders = pp.unique_orders(test_data)\n",
    "\n",
    "    train_unique = list(train_data['ArticleName'].unique())\n",
    "\n",
    "    all_unique = pd.concat([train_data, test_data], ignore_index=True, axis=0)['ArticleName'].unique() \n",
    "\n",
    "    train_df, train_supports = get_arm_data(train_orders, list(all_unique))\n",
    "    test_df, _ = get_arm_data(test_orders, list(all_unique))\n",
    "\n",
    "\n",
    "\n",
    "    test_df = test_df.reindex(train_df.columns, axis=1)\n",
    "\n",
    "    article_list_ordered = list(train_df.columns)\n",
    "\n",
    "    vectorizer = pp.Vectorizer(article_list_ordered)\n",
    "\n",
    "    bow = vectorizer.get_sparse_bow_embeddings(list(all_unique), save_path=f'../Data/sparse_bow_{n_orders}_cv{cv_index}.pkl')\n",
    "\n",
    "    relevant_data = [train_df, test_df, train_supports, train_unique, all_unique, train_orders, vectorizer]\n",
    "    f = open(f'../Data/{n_orders}_orders/data_{cv_index}.pkl', \"wb\")\n",
    "    pickle.dump(relevant_data, f)\n",
    "    f.close()\n",
    "    return relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    f = open(filepath, \"rb\")\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = create_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy(articles, \n",
    "               layout=layout, \n",
    "               distance_weight=1, \n",
    "               rule_weight=0.01, \n",
    "               rule_weight_for_article_scores=1e4, \n",
    "               penalty_weight=0, \n",
    "               verbose=False,\n",
    "               hp_tuning=False):\n",
    "\n",
    "    main_model = Greedy(layout=layout, \n",
    "                        articles=articles, \n",
    "                        distance_weight=distance_weight, \n",
    "                        rule_weight=rule_weight, \n",
    "                        rule_weight_for_article_scores=rule_weight_for_article_scores, \n",
    "                        penalty_weight=penalty_weight,\n",
    "                        hp_tuning=hp_tuning)\n",
    "\n",
    "    warehouse = Warehouse(layout, main_model)\n",
    "\n",
    "    solution_matrix, _, product_to_shelves = warehouse.optimize_locations() \n",
    "    \n",
    "    if verbose:\n",
    "        sns.heatmap(solution_matrix, square=True) \n",
    "        plt.show()  \n",
    "\n",
    "    return product_to_shelves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genetic(articles,\n",
    "                layout=layout, \n",
    "                distance_weight=1,\n",
    "                rule_weight=1, \n",
    "                population_size=100, \n",
    "                n_iter=100, \n",
    "                crossover_rate=1, \n",
    "                mutation_rate=0.1, \n",
    "                k_selection=3, \n",
    "                crossover='crossover_height', \n",
    "                fitness='fitness_distance_and_rules', \n",
    "                orders=None, \n",
    "                warm_start=False, \n",
    "                parallel=False,\n",
    "                verbose=0,\n",
    "                solution=None):\n",
    "\n",
    "    main_model = GeneticModel(articles=articles,\n",
    "                              layout=layout, \n",
    "                              rule_weight=rule_weight, \n",
    "                              distance_weight=distance_weight,\n",
    "                              population_size=population_size, \n",
    "                              n_iter=n_iter, \n",
    "                              crossover_rate=crossover_rate, \n",
    "                              mutation_rate=mutation_rate,\n",
    "                              k_selection=k_selection, \n",
    "                              crossover=crossover, \n",
    "                              fitness=fitness,\n",
    "                              orders=orders,\n",
    "                              warm_start=warm_start,\n",
    "                              parallel=parallel,\n",
    "                              verbose=verbose,\n",
    "                              solution=solution)\n",
    "\n",
    "    warehouse = Warehouse(layout, main_model)\n",
    "\n",
    "    solution_matrix, best_scores, product_to_shelves = warehouse.optimize_locations() \n",
    "    \n",
    "    if verbose > 0:\n",
    "        plt.figure(1)\n",
    "        sns.heatmap(solution_matrix, square=True) \n",
    "        plt.show() \n",
    "        plt.figure(2)\n",
    "        plt.plot(np.arange(len(best_scores)), best_scores) \n",
    "        plt.show() \n",
    "\n",
    "        \n",
    "    return product_to_shelves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ear_rules(ar_rules, \n",
    "                  unique_items,\n",
    "                  k=4,\n",
    "                  r=1, \n",
    "                  embeddings_path=f'../Data/sparse_bow_{n_orders}_cv0.pkl', \n",
    "                  save_path=None, \n",
    "                  word_weights=defaultdict(lambda: 1), \n",
    "                  is_sparse=True,\n",
    "                  beta=1/10,\n",
    "                  parallel_rules=False,\n",
    "                  parallel_weight=1):\n",
    "    if save_path is None:\n",
    "        save_path = f'../ear_rules.pkl'\n",
    "    try:\n",
    "        f = open(save_path, \"rb\")\n",
    "        ear_rules = pickle.load(f)\n",
    "        f.close()\n",
    "    except:\n",
    "        ear_rules = compute_ear_rules(ar_rules, \n",
    "                                        unique_items=unique_items,\n",
    "                                        k=k,\n",
    "                                        radius=r, \n",
    "                                        embeddings_path=embeddings_path, \n",
    "                                        save_path=save_path, \n",
    "                                        word_weights=word_weights, \n",
    "                                        is_sparse=is_sparse,\n",
    "                                        beta=beta,\n",
    "                                        parallel_rules=parallel_rules,\n",
    "                                        parallel_weight=parallel_weight) \n",
    "    return ear_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(train_path, test_path, save_path):\n",
    "\n",
    "    d1, d2, d3 = pp.train_val_test_split(data_path=train_path, \n",
    "                                                        val_prop=0.25, test_prop=0.25, \n",
    "                                                        n_orders=10000000)\n",
    "\n",
    "    train_data = pd.concat([d1, d2, d3], ignore_index=True, axis=0) \n",
    "\n",
    "\n",
    "    d1, d2, d3 = pp.train_val_test_split(data_path=test_path, \n",
    "                                                        val_prop=0.25, test_prop=0.25, \n",
    "                                                        n_orders=10000000)\n",
    "\n",
    "    test_data = pd.concat([d1, d2, d3], ignore_index=True, axis=0) \n",
    "\n",
    "\n",
    "                                    \n",
    "                            \n",
    "\n",
    "    train_data = pp.preprocess_data(train_data)\n",
    "    test_data = pp.preprocess_data(test_data)\n",
    "\n",
    "\n",
    "    train_orders = pp.unique_orders(train_data)\n",
    "    test_orders = pp.unique_orders(test_data)\n",
    "\n",
    "    train_unique = list(train_data['ArticleName'].unique())\n",
    "\n",
    "    all_unique = pd.concat([train_data, test_data], ignore_index=True, axis=0)['ArticleName'].unique() \n",
    "\n",
    "    train_df, train_supports = get_arm_data(train_orders, list(all_unique))\n",
    "    test_df, _ = get_arm_data(test_orders, list(all_unique))\n",
    "\n",
    "\n",
    "\n",
    "    test_df = test_df.reindex(train_df.columns, axis=1)\n",
    "\n",
    "    article_list_ordered = list(train_df.columns)\n",
    "\n",
    "    vectorizer = pp.Vectorizer(article_list_ordered)\n",
    "\n",
    "\n",
    "\n",
    "    bow = vectorizer.get_sparse_bow_embeddings(list(all_unique), save_path=f'../Data/sparse_bow.pkl')\n",
    "\n",
    "\n",
    "\n",
    "    relevant_data = [train_df, test_df, train_supports, train_unique, all_unique, train_orders, vectorizer]\n",
    "    f = open(save_path, \"wb\")\n",
    "    pickle.dump(relevant_data, f)\n",
    "    f.close()\n",
    "    return relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'data_path.pkl', \"rb\")\n",
    "data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "train_df, test_df, train_supports, train_unique, all_unique, train_orders, vectorizer = data\n",
    "bow = vectorizer.get_sparse_bow_embeddings(list(all_unique), save_path=f'../Data/sparse_bow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_supports = train_supports.copy()\n",
    "for article in random_supports.keys():\n",
    "    random_supports[article] = np.random.rand()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM BASELINE\n",
    "random_results = []\n",
    "for _ in range(5):\n",
    "    for article in random_supports.keys():\n",
    "        random_supports[article] = np.random.rand()\n",
    "    train_articles = create_articles(random_supports, ar_weight=0, ear_weight=0, nn_weight=0)\n",
    "\n",
    "    solution = run_greedy(articles=train_articles, layout=layout, distance_weight=1, rule_weight=0, rule_weight_for_article_scores=0, penalty_weight=0, verbose=False, hp_tuning=False)\n",
    "\n",
    "    average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "    random_results.append(average_distance_travelled)\n",
    "print(f'{np.mean(random_results)} +- {np.std(random_results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN ORDER BASELINE\n",
    "train_articles = create_articles(train_supports, ar_weight=0, ear_weight=0, nn_weight=0)\n",
    "solution = run_greedy(articles=train_articles, layout=layout, distance_weight=1, rule_weight=0, rule_weight_for_article_scores=0, penalty_weight=0, verbose=False, hp_tuning=False)\n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Baseline:', average_distance_travelled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_rows(n_workers, orders):\n",
    "    order_list = []\n",
    "    orders2 = list(orders['Articles'])\n",
    "    \n",
    "    if len(orders2) % n_workers == 0:\n",
    "        # Dividable\n",
    "        size = len(orders2) // n_workers\n",
    "        \n",
    "        for i in range(n_workers):\n",
    "            row = orders2[i*size:(i+1)*size]\n",
    "            order_list.append(row)\n",
    "\n",
    "        return order_list\n",
    "\n",
    "    else:\n",
    "        n_over = len(orders2) % n_workers\n",
    "        size = len(orders2) // n_workers\n",
    "        \n",
    "        for i in range(n_workers):\n",
    "            row = orders2[i*size:(i+1)*size]\n",
    "            order_list.append(row)\n",
    "        \n",
    "        for j in range(n_over):\n",
    "            idx = j+1\n",
    "            order = orders2[-idx]\n",
    "            order_list[j].append(order)\n",
    "\n",
    "        return order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parallell\n",
    "# List for storing the results\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "### Callback function for collecting the results\n",
    "def callback_train(result):\n",
    "    global train_results\n",
    "    train_results.append(result)\n",
    "\n",
    "\n",
    "def callback_test(result):\n",
    "    global test_results\n",
    "    test_results.append(result)\n",
    "\n",
    "\n",
    "print(train_orders.shape)\n",
    "\n",
    "st_par = time.time()\n",
    "# Needs main to function\n",
    "if __name__ == '__main__':\n",
    "    print('started')\n",
    "    p_train = Pool(8) # A pool of 8 processes to run at the same time\n",
    "    s_train = time.time()\n",
    "\n",
    "    # Split orders into 8 parts\n",
    "    order_list_train = split_into_rows(8, train_orders)\n",
    "    for orders in order_list_train:\n",
    "        p_train.apply_async(fast_vectorize_data, args=(vectorizer, orders), callback=callback_train)\n",
    "    \n",
    "    # Close the processes\n",
    "    p_train.close()\n",
    "    p_train.join()\n",
    "    print(f'Parallell train time: {time.time() - s_train}, n=15000\\n')\n",
    "\n",
    "t = []\n",
    "for tr in train_results:\n",
    "    t += tr\n",
    "    \n",
    "X_train = np.asarray(t, dtype=np.int8)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[3])\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "X_train_file = open(f'../Data/X_train.pkl', 'wb')\n",
    "pickle.dump(X_train, X_train_file)\n",
    "X_train_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_shape = X_train.shape\n",
    "article_shape = X_shape[-1] \n",
    "\n",
    "embedding_dim = 256\n",
    "alpha = 1.0\n",
    "w_path = f'../Data/f_theta.h5' \n",
    "\n",
    "f_train = tm.F_theta(article_shape=(article_shape), train_predict=0, \n",
    "                    embedding_dim=embedding_dim, separation=alpha,\n",
    "                    weights_path=w_path, visualize=False, verbose=False)\n",
    "\n",
    "\n",
    "clean = False\n",
    "f_train.train(X_train=X_train[:-10000,:,:], \n",
    "              y_train=X_train[:-10000,:,:], \n",
    "              val_data=([X_train[-10000:,0,:], X_train[-10000:,1,:], X_train[-10000:,2,:]], X_train[-10000:,0,:]), \n",
    "              num_epochs=3,\n",
    "              batch_size=1024, \n",
    "              num_workers=8, \n",
    "              early_stopping=False)\n",
    "if clean:\n",
    "    del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_pred = tm.F_theta(article_shape=(article_shape), train_predict=1,\n",
    "                    embedding_dim=embedding_dim, separation=alpha,\n",
    "                    weights_path=w_path, visualize=False, verbose=False)\n",
    "\n",
    "# Create embeddings\n",
    "article_names, article_vecs = vectorizer.get_articles_for_embedding([])\n",
    "embedding_space = np.asarray(f_pred.network(article_vecs))\n",
    "embedding_map = {}\n",
    "\n",
    "# Map article name to embedding\n",
    "for name, embedding in zip(article_names, embedding_space):\n",
    "    embedding_map[name] = embedding\n",
    "\n",
    "\n",
    "\n",
    "nn_rules = compute_nn_rules(train_supports,\n",
    "                            embedding_space=embedding_space,\n",
    "                            embedding_map=embedding_map, \n",
    "                            k=100,\n",
    "                            r=1e-3,\n",
    "                            save_path='../Data/nn_rules.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR results!\n",
    "dw = 1e5\n",
    "rw = 1\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, ar_rules=ar_rules, ar_weight=1, ear_weight=0, nn_weight=0)\n",
    "solution = run_genetic(articles=train_articles,\n",
    "                layout=layout, \n",
    "                distance_weight=dw,\n",
    "                rule_weight=rw, \n",
    "                population_size=100, \n",
    "                n_iter=100, \n",
    "                crossover_rate=100, \n",
    "                mutation_rate=0.001, \n",
    "                k_selection=3, \n",
    "                crossover='crossover_height', \n",
    "                fitness='fitness_distance_and_rules', \n",
    "                orders=None, \n",
    "                warm_start=True, \n",
    "                parallel=False,\n",
    "                verbose=1,\n",
    "                solution=None)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted EAR results!\n",
    "dw = 3e8\n",
    "rw = 1\n",
    "\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "\n",
    "\n",
    "parallel_weight = 1000\n",
    "k = 6\n",
    "r = 6\n",
    "\n",
    "\n",
    "words = vectorizer.cv.get_feature_names_out()\n",
    "\n",
    "ear_rules = get_ear_rules(ar_rules, \n",
    "                            unique_items=list(all_unique),\n",
    "                            k=k,\n",
    "                            r=r, \n",
    "                            embeddings_path=f'../Data/sparse_bow.pkl', \n",
    "                            save_path=f'../Data/ear_rules_unweeighted.pkl', \n",
    "                            word_weights=np.ones_like(words), \n",
    "                            is_sparse=True,\n",
    "                            beta=r/10,\n",
    "                            parallel_rules=True,\n",
    "                            parallel_weight=parallel_weight) \n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, ear_rules=ear_rules, ar_weight=0, ear_weight=1, nn_weight=0)\n",
    "solution = run_genetic(articles=train_articles,\n",
    "                layout=layout, \n",
    "                distance_weight=dw,\n",
    "                rule_weight=rw, \n",
    "                population_size=100, \n",
    "                n_iter=5, \n",
    "                crossover_rate=100, \n",
    "                mutation_rate=0.001, \n",
    "                k_selection=3, \n",
    "                crossover='crossover_height', \n",
    "                fitness='fitness_distance_and_rules', \n",
    "                orders=None, \n",
    "                warm_start=True, \n",
    "                parallel=False,\n",
    "                verbose=1,\n",
    "                solution=None)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAR results!\n",
    "dw = 5e6\n",
    "rw = 1\n",
    "\n",
    "gamma = 1.6\n",
    "delta = 3.5\n",
    "parallel_weight = 10\n",
    "k = 2\n",
    "r = 1\n",
    "\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "words = vectorizer.cv.get_feature_names_out()\n",
    "word_scores, single_word_scores, word_frequencies = calculate_word_scores(train_unique, \n",
    "                                                                            train_orders, \n",
    "                                                                            words, \n",
    "                                                                            gamma=gamma,\n",
    "                                                                            delta=delta)\n",
    "                                                                            \n",
    "min_score = np.min([score for score in word_scores if score > 0])\n",
    "word_scores = [score if score > 0 else min_score/2 for score in word_scores]\n",
    "ear_rules = get_ear_rules(ar_rules, \n",
    "                            unique_items=list(all_unique),\n",
    "                            k=k,\n",
    "                            r=r, \n",
    "                            embeddings_path=f'../Data/sparse_bow.pkl', \n",
    "                            save_path=f'../Data/ear_rules_pair_scores.pkl', \n",
    "                            word_weights=word_scores, \n",
    "                            is_sparse=True,\n",
    "                            beta=r/10,\n",
    "                            parallel_rules=True,\n",
    "                            parallel_weight=parallel_weight) \n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, ear_rules=ear_rules, ar_weight=0, ear_weight=1, nn_weight=0)\n",
    "solution = run_genetic(articles=train_articles,\n",
    "                layout=layout, \n",
    "                distance_weight=dw,\n",
    "                rule_weight=rw, \n",
    "                population_size=100, \n",
    "                n_iter=5, \n",
    "                crossover_rate=100, \n",
    "                mutation_rate=0.001, \n",
    "                k_selection=3, \n",
    "                crossover='crossover_height', \n",
    "                fitness='fitness_distance_and_rules', \n",
    "                orders=None, \n",
    "                warm_start=True, \n",
    "                parallel=False,\n",
    "                verbose=1,\n",
    "                solution=None)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNR results!\n",
    "dw = 1e5\n",
    "rw = 1\n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, nn_rules=nn_rules, ar_weight=0, ear_weight=0, nn_weight=1)\n",
    "solution = run_genetic(articles=train_articles,\n",
    "                layout=layout, \n",
    "                distance_weight=dw,\n",
    "                rule_weight=rw, \n",
    "                population_size=100, \n",
    "                n_iter=5000, \n",
    "                crossover_rate=100, \n",
    "                mutation_rate=0.001, \n",
    "                k_selection=3, \n",
    "                crossover='crossover_height', \n",
    "                fitness='fitness_distance_and_rules', \n",
    "                orders=None, \n",
    "                warm_start=True, \n",
    "                parallel=False,\n",
    "                verbose=1,\n",
    "                solution=None)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rules results!\n",
    "dw = 5e6\n",
    "rw = 1\n",
    "\n",
    "gamma = 1.6\n",
    "delta = 3.5\n",
    "parallel_weight = 10\n",
    "k = 2\n",
    "r = 1\n",
    "\n",
    "ear_weight = 1e-4\n",
    "nn_weight = 10\n",
    "\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "words = vectorizer.cv.get_feature_names_out()\n",
    "word_scores, single_word_scores, word_frequencies = calculate_word_scores(train_unique, \n",
    "                                                                            train_orders, \n",
    "                                                                            words, \n",
    "                                                                            gamma=gamma,\n",
    "                                                                            delta=delta)\n",
    "                                                                            \n",
    "min_score = np.min([score for score in word_scores if score > 0])\n",
    "word_scores = [score if score > 0 else min_score/2 for score in word_scores]\n",
    "ear_rules = get_ear_rules(ar_rules, \n",
    "                            unique_items=list(all_unique),\n",
    "                            k=k,\n",
    "                            r=r, \n",
    "                            embeddings_path=f'../Data/sparse_bow.pkl', \n",
    "                            save_path=f'../Data/ear_rules_pair_scores.pkl', \n",
    "                            word_weights=word_scores, \n",
    "                            is_sparse=True,\n",
    "                            beta=r/10,\n",
    "                            parallel_rules=True,\n",
    "                            parallel_weight=parallel_weight) \n",
    "\n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, ar_rules=ar_rules, ear_rules=ear_rules, nn_rules=nn_rules, ar_weight=1, ear_weight=ear_weight, nn_weight=nn_weight)\n",
    "solution = run_genetic(articles=train_articles,\n",
    "                layout=layout, \n",
    "                distance_weight=dw,\n",
    "                rule_weight=rw, \n",
    "                population_size=100, \n",
    "                n_iter=5000, \n",
    "                crossover_rate=100, \n",
    "                mutation_rate=0.001, \n",
    "                k_selection=3, \n",
    "                crossover='crossover_height', \n",
    "                fitness='fitness_distance_and_rules', \n",
    "                orders=None, \n",
    "                warm_start=True, \n",
    "                parallel=False,\n",
    "                verbose=1,\n",
    "                solution=None)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAR results!\n",
    "parallel_weight = 10\n",
    "gamma = 1.6\n",
    "delta = 3.5\n",
    "k = 2\n",
    "r = 1\n",
    "\n",
    "dw = 5e5\n",
    "aw = 1\n",
    "p = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "words = vectorizer.cv.get_feature_names_out()\n",
    "word_scores, single_word_scores, word_frequencies = calculate_word_scores(train_unique, \n",
    "                                                                            train_orders, \n",
    "                                                                            words, \n",
    "                                                                            gamma=gamma,\n",
    "                                                                            delta=delta)\n",
    "                                                                            \n",
    "min_score = np.min([score for score in word_scores if score > 0])\n",
    "word_scores = [score if score > 0 else min_score/2 for score in word_scores]\n",
    "ear_rules = get_ear_rules(ar_rules, \n",
    "                            unique_items=list(all_unique),\n",
    "                            k=k,\n",
    "                            r=r, \n",
    "                            embeddings_path=f'../Data/sparse_bow.pkl', \n",
    "                            save_path=f'../Data/ear_rules_pair_scores.pkl', \n",
    "                            word_weights=word_scores, \n",
    "                            is_sparse=True,\n",
    "                            beta=r/10,\n",
    "                            parallel_rules=True,\n",
    "                            parallel_weight=parallel_weight) \n",
    "\n",
    "train_articles = create_articles(train_supports, ear_rules=ear_rules, ar_weight=0, ear_weight=1, nn_weight=0)\n",
    "solution = run_greedy(articles=train_articles, layout=layout, distance_weight=dw, rule_weight=1, rule_weight_for_article_scores=aw, penalty_weight=p, verbose=False, hp_tuning=False)\n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted EAR results!\n",
    "parallel_weight = 1000\n",
    "\n",
    "k = 6\n",
    "r = 6\n",
    "\n",
    "dw = 1e8\n",
    "aw = 1\n",
    "p = 0\n",
    "\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "\n",
    "ear_rules = get_ear_rules(ar_rules, \n",
    "                            unique_items=list(all_unique),\n",
    "                            k=k,\n",
    "                            r=r, \n",
    "                            embeddings_path=f'../Data/sparse_bow.pkl', \n",
    "                            save_path=f'../Data/ear_rules_unweighted.pkl', \n",
    "                            word_weights=defaultdict(lambda: 1), \n",
    "                            is_sparse=True,\n",
    "                            beta=r/10,\n",
    "                            parallel_rules=True,\n",
    "                            parallel_weight=parallel_weight) \n",
    "\n",
    "train_articles = create_articles(train_supports, ear_rules=ear_rules, ar_weight=0, ear_weight=1, nn_weight=0)\n",
    "solution = run_greedy(articles=train_articles, layout=layout, distance_weight=dw, rule_weight=1, rule_weight_for_article_scores=aw, penalty_weight=p, verbose=False, hp_tuning=False)\n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR results!\n",
    "dw = 5e4\n",
    "aw = 1\n",
    "p = 0\n",
    "\n",
    "scores = defaultdict(list)\n",
    "\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, ar_rules=ar_rules, ar_weight=1, ear_weight=0, nn_weight=0)\n",
    "solution = run_greedy(articles=train_articles, layout=layout, distance_weight=dw, rule_weight=1, rule_weight_for_article_scores=aw, penalty_weight=p, verbose=False, hp_tuning=False)\n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNR\n",
    "dw = 1e4\n",
    "aw = 1\n",
    "p = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, nn_rules=nn_rules, ar_weight=0, ear_weight=0, nn_weight=1)\n",
    "solution = run_greedy(articles=train_articles, layout=layout, distance_weight=dw, rule_weight=1, rule_weight_for_article_scores=aw, penalty_weight=p, verbose=False, hp_tuning=False)\n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rules together\n",
    "\n",
    "parallel_weight = 10\n",
    "gamma = 1.6\n",
    "delta = 3.5\n",
    "k = 2\n",
    "r = 1\n",
    "\n",
    "ear_weight = 1e-4\n",
    "nn_weight = 10\n",
    "\n",
    "dw = 5e5\n",
    "aw = 1\n",
    "p = 0\n",
    "\n",
    "\n",
    "\n",
    "itemsets, itemsets_size_2 = generate_itemsets(train_df, min_support=0.0001)\n",
    "\n",
    "ar_rules = compute_ar_rules(itemsets_size_2, metric='lift', min_threshold=1, \n",
    "                            save_path=f'../Data/ar_rules.pkl')\n",
    "\n",
    "words = vectorizer.cv.get_feature_names_out()\n",
    "word_scores, single_word_scores, word_frequencies = calculate_word_scores(train_unique, \n",
    "                                                                            train_orders, \n",
    "                                                                            words, \n",
    "                                                                            gamma=gamma,\n",
    "                                                                            delta=delta)\n",
    "                                                                            \n",
    "min_score = np.min([score for score in word_scores if score > 0])\n",
    "word_scores = [score if score > 0 else min_score/2 for score in word_scores]\n",
    "ear_rules = get_ear_rules(ar_rules, \n",
    "                            unique_items=list(all_unique),\n",
    "                            k=k,\n",
    "                            r=r, \n",
    "                            embeddings_path=f'../Data/sparse_bow.pkl', \n",
    "                            save_path=f'../Data/ear_rules_pair_scores.pkl', \n",
    "                            word_weights=word_scores, \n",
    "                            is_sparse=True,\n",
    "                            beta=r/10,\n",
    "                            parallel_rules=True,\n",
    "                            parallel_weight=parallel_weight) \n",
    "\n",
    "\n",
    "train_articles = create_articles(train_supports, ar_rules=ar_rules, ear_rules=ear_rules, nn_rules=nn_rules, ar_weight=1, ear_weight=ear_weight, nn_weight=nn_weight)\n",
    "solution = run_greedy(articles=train_articles, layout=layout, distance_weight=dw, rule_weight=1, rule_weight_for_article_scores=aw, penalty_weight=p, verbose=False, hp_tuning=False)\n",
    "average_distance_travelled = evaluate_solution_for_greedy(solution, test_df, layout, batch_size=1, verbose=True)\n",
    "print('Result:', average_distance_travelled)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4fdbf519b56afa0b6c543e4f63545820f38d96cef28717ca11396e5e9a19357"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
